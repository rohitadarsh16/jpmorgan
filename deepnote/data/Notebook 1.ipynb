{
  "cells": [
    {
      "cell_type": "code",
      "source": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport aiohttp\nimport asyncio\nimport re\nfrom aiofiles import open as aio_open\nfrom asyncio import Lock\nimport nest_asyncio\nnest_asyncio.apply()\n\n\ndf = pd.read_excel(\"companiesDropdown.xlsx\")\nfile_lock = Lock()\nasync def to_excel(df, filename):\n    loop = asyncio.get_event_loop()\n    async with file_lock:\n        await loop.run_in_executor(None, lambda: df.to_excel(filename, index=False))\n\n\n\n\n\n# async def fetch(session, url, params=None, data=None, headers=None):\n#     async with session.post(url, params=params, data=data, headers=headers) as response:\n#         return await response.text()\n        \nasync def fetch(session, url, params=None, data=None, headers=None, retries=3):\n    for attempt in range(retries):\n        try:\n            async with session.post(url, params=params, data=data, headers=headers) as response:\n                return await response.text()\n        except (aiohttp.ClientResponseError, aiohttp.client_exceptions.ServerDisconnectedError) as e:\n            print(f\"Error: {e}. Retrying {attempt + 1}/{retries}\")\n            await asyncio.sleep(2)  # Wait for a short duration before retrying\n    return None\n\n\n\nasync def get_last_page(session, fund_id, page):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-GB,en;q=0.5',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Origin': 'https://central-webd.proxydisclosure.com',\n        'Connection': 'keep-alive',\n        'Referer': 'https://central-webd.proxydisclosure.com/WebDisclosure/wdMeetingList',\n        'Upgrade-Insecure-Requests': '1',\n        'Sec-Fetch-Dest': 'document',\n        'Sec-Fetch-Mode': 'navigate',\n        'Sec-Fetch-Site': 'same-origin',\n        'Sec-Fetch-User': '?1',\n        'Pragma': 'no-cache',\n        'Cache-Control': 'no-cache',\n    }\n\n    data = {\n        'siteId': 'JPMFunds',\n        'fundId': fund_id,\n        'fundIdTmp': '',\n        'previewkey': '',\n        'currentPageNumber': str(page),\n        'meetingId': '',\n        'sortByColumn': 'COMPANY_NAME',\n        'sortingOrder': 'ASC',\n        'fundCompNameSection': '',\n        'tickerSymbol': '',\n        'companyName': '',\n        'companyNameStartsWith': '',\n        'meetingDate': '',\n        'meetingTypeDesc': '',\n        'securityId': '',\n        'tickerSymbolPage3': '',\n        'isin': '',\n        'compNamePage2To3': '',\n    }\n\n    response = await fetch(session, \"https://central-webd.proxydisclosure.com/WebDisclosure/wdMeetingList\", data=data, headers=headers)\n\n    soup = BeautifulSoup(response, 'html.parser')\n    page_div = soup.find('div', id='pageNbrText')\n    if page_div:\n        parts = page_div.text.split()\n        last_page = parts[-1] if len(parts) > 1 else parts[0]\n        last_page_number = int(last_page)\n        if last_page == '1':\n            data_present = soup.find('td')\n            print(data_present.text)\n            if data_present.text == 'No Data Found':\n                last_page_number = 0\n    else:\n        last_page_number = 0\n    return last_page_number\nbackup_data = []\nasync def get_allpage_data(session, fund_company, fund_id, sheet_data):\n    global fund_not_has_data\n    global backup_data\n    last_page_num = await get_last_page(session, fund_id, '1')\n    results = []\n    print(\"Last Page Number:\", last_page_num)\n    for page in range(1, int(last_page_num) + 1):\n        page_results = await get_page_data(session, fund_company, fund_id, str(page), sheet_data)\n        backup_data.extend(page_results)\n        backup_df = pd.DataFrame(backup_data)\n        await to_excel(backup_df, \"backup.xlsx\")\n        results.extend(page_results)\n    return results\n\nasync def get_page_data(session, fund_company, fund_id, page, sheet_data):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-GB,en;q=0.5',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Origin': 'https://central-webd.proxydisclosure.com',\n        'Connection': 'keep-alive',\n        'Referer': 'https://central-webd.proxydisclosure.com/WebDisclosure/wdMeetingList',\n        'Upgrade-Insecure-Requests': '1',\n        'Sec-Fetch-Dest': 'document',\n        'Sec-Fetch-Mode': 'navigate',\n        'Sec-Fetch-Site': 'same-origin',\n        'Sec-Fetch-User': '?1',\n        'Pragma': 'no-cache',\n        'Cache-Control': 'no-cache',\n    }\n\n    data = {\n        'siteId': 'JPMFunds',\n        'fundId': fund_id,\n        'fundIdTmp': '',\n        'previewkey': '',\n        'currentPageNumber': str(page),\n        'meetingId': '',\n        'sortByColumn': 'COMPANY_NAME',\n        'sortingOrder': 'ASC',\n        'fundCompNameSection': '',\n        'tickerSymbol': '',\n        'companyName': '',\n        'companyNameStartsWith': '',\n        'meetingDate': '',\n        'meetingTypeDesc': '',\n        'securityId': '',\n        'tickerSymbolPage3': '',\n        'isin': '',\n        'compNamePage2To3': '',\n    }\n\n    response = await fetch(session, 'https://central-webd.proxydisclosure.com/WebDisclosure/wdMeetingList', data=data, headers=headers)\n\n    soup = BeautifulSoup(response, 'html.parser')\n    with open('index.html', 'w', encoding='utf-8') as f:\n        f.write(response)\n    td_elements = soup.find_all('td')\n    results = []\n    tasks = []\n    for td in td_elements:\n        anchor_tag = td.find('a')\n        if anchor_tag:\n            company = anchor_tag.text.strip()\n            href_parts = re.split(r\"(?<!\\\\)'\", anchor_tag['href'])[1::2]\n            if len(href_parts) == 7:\n                meeting_id, meeting_date, meeting_type_desc, security_id, ticker_symbol_page3, isin, comp_name_page2to3 = href_parts\n                comp_name_page2to3 = comp_name_page2to3.strip()\n            else:\n                meeting_id, meeting_date, meeting_type_desc, security_id, ticker_symbol_page3, isin, comp_name_page2to3 = anchor_tag['href'].split(\"'\")[1::2]\n                comp_name_page2to3 = comp_name_page2to3.strip()\n\n            result_id = {\n                'fundId': fund_id,\n                'meetingId': meeting_id,\n                'meetingDate': meeting_date,\n                'meetingTypeDesc': meeting_type_desc,\n                'securityId': security_id,\n                'isin': isin,\n                'compNamePage2To3': comp_name_page2to3\n            }\n             # Instead of calling get_company_data directly, create a task\n            task = get_company_data(session, result_id, fund_company, company, sheet_data)\n            tasks.append(task)\n\n    # Use asyncio.gather to run the tasks in parallel\n    results = await asyncio.gather(*tasks)\n    # print(results)\n    \n    return results\n\nasync def get_company_data(session, dict_ids, fund_company, company_name, sheet_data):\n    global fund_not_has_data\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-GB,en;q=0.5',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Origin': 'https://central-webd.proxydisclosure.com',\n        'Connection': 'keep-alive',\n        'Referer': 'https://central-webd.proxydisclosure.com/WebDisclosure/wdMeetingList',\n        'Upgrade-Insecure-Requests': '1',\n        'Sec-Fetch-Dest': 'document',\n        'Sec-Fetch-Mode': 'navigate',\n        'Sec-Fetch-Site': 'same-origin',\n        'Sec-Fetch-User': '?1',\n        'Pragma': 'no-cache',\n        'Cache-Control': 'no-cache',\n    }\n\n    data = {\n        'siteId': 'JPMFunds',\n        'fundId': dict_ids['fundId'],\n        'fundIdTmp': '',\n        'previewkey': '',\n        'meetingId': dict_ids['meetingId'],\n        'sortByColumn': 'COMPANY_NAME',\n        'sortingOrder': 'ASC',\n        'fundCompNameSection': '',\n        'tickerSymbol': '',\n        'companyName': '',\n        'companyNameStartsWith': '',\n        'meetingDate': dict_ids['meetingDate'],\n        'meetingTypeDesc': dict_ids['meetingTypeDesc'],\n        'securityId': dict_ids['securityId'],\n        'tickerSymbolPage3': '',\n        'isin': dict_ids['isin'],\n        'compNamePage2To3': dict_ids['compNamePage2To3'],\n    }\n\n    response = await fetch(session, \"https://central-webd.proxydisclosure.com/WebDisclosure/wdMeetingDetail\", data=data, headers=headers)\n\n    soup = BeautifulSoup(response, 'html.parser')\n    table = soup.find('table')\n    data_dict = {}\n    labels = table.find_all('label', class_='data-field')\n    company_data_list = []\n    tables = soup.find('table', class_='tbl')\n    if tables:\n        second_table = tables\n        for row in second_table.find_all('tr')[1:]:\n            columns = row.find_all('td')\n            item, proposal_description, proposal_type, vote, management_recommendation = [column.text.strip() for column in columns]\n            data_dict['company'] = company_name\n            data_dict['Meeting Date'] = labels[0].text\n            data_dict['Meeting Type'] = labels[1].text\n            data_dict['Security/CINS'] = labels[2].text\n            data_dict['Ticker'] = labels[3].text\n            data_dict['Agenda Number'] = labels[4].text\n            data_dict['ISIN'] = labels[5].text\n            data_dict['Item'] = item\n            data_dict['Proposal Description'] = proposal_description\n            data_dict['Proposal Type'] = proposal_type\n            data_dict['Vote'] = vote\n            data_dict['Management Recommendation'] = management_recommendation\n            data_dict['Fund Company'] = fund_company\n            company_data_list.append(data_dict)\n            print(data_dict)\n        sheet_data.extend(company_data_list)\n        data_sheet = pd.DataFrame(sheet_data)\n        capitalize_first_letter = lambda x: x.str.capitalize() if x.dtype == 'object' else x\n        data_sheet = data_sheet.apply(capitalize_first_letter)\n        if fund_not_has_data:\n            data_proxy = pd.DataFrame(fund_not_has_data)\n            data_sheet = pd.concat([data_sheet, data_proxy], axis=1)\n       \n        await to_excel(data_sheet, \"extracted.xlsx\")\n    else:\n        print(\"There are not enough tables.\")\n\n    return data_dict\ntotalData = []\nasync def process_fund_data(session, fund_company, fund_id, sheet_data, fund_not_has_data):\n    print(f\"Processing {fund_company}, Fund ID: {fund_id}\")\n\n    all_data = await get_allpage_data(session, fund_company, fund_id, sheet_data)\n\n    if all_data:\n        # Create a single DataFrame for all data\n        totalData.extend(all_data)\n        total_data_df = pd.DataFrame(totalData)\n\n        # Save the DataFrame to Excel using aiofiles for asynchronous file writing\n        await to_excel(total_data_df, \"totaldata.xlsx\")\n    else:\n        print(\"There are not enough data.\")\n        fund_not_has_data.append(fund_company)\n        data_proxy = pd.DataFrame(fund_not_has_data)\n        await to_excel(data_proxy, \"prroxy.xlsx\")\n\nsheet_data = []\nfund_not_has_data = []\nasync def main():\n    global sheet_data\n    global fund_not_has_data\n    connector = aiohttp.TCPConnector(limit=10)\n    async with aiohttp.ClientSession(connector=connector) as session:\n        for fund_id in df['Fund']:\n            if fund_id == '0' or fund_id == '@@-$$-@@-$$-@@':\n                continue\n            fund_company = df[df['Fund'] == fund_id]['Company Name'].values[0]\n            await process_fund_data(session, fund_company, fund_id, sheet_data, fund_not_has_data)\n\n# Call the main function using asyncio.run() outside the cell\nasyncio.run(main())",
      "metadata": {
        "source_hash": "493f6dc4",
        "execution_start": 1703697865124,
        "execution_millis": 462,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "c9c88387f76d434984da25208d37b8b8",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'aiohttp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiohttp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aiohttp'"
          ]
        }
      ],
      "execution_count": 2,
      "block_group": "c9c88387f76d434984da25208d37b8b8"
    },
    {
      "cell_type": "code",
      "source": "!pip install requests\n!pip install beautifulsoup4\n!pip install pandas\n!pip install aiohttp\n!pip install aiofiles\n!pip install openpyxl",
      "metadata": {
        "source_hash": "ba02072e",
        "execution_start": 1703697824409,
        "execution_millis": 40706,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "6ec16e5b0d93416b910db40350b32ea7",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: requests in /shared-libs/python3.9/py/lib/python3.9/site-packages (2.28.1)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests) (1.26.12)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: beautifulsoup4 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (4.11.1)\nRequirement already satisfied: soupsieve>1.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from beautifulsoup4) (2.3.2.post1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: pandas in /shared-libs/python3.9/py/lib/python3.9/site-packages (1.2.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas) (2022.5)\nRequirement already satisfied: numpy>=1.16.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas) (1.23.4)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting aiohttp\n  Downloading aiohttp-3.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<5.0,>=4.0\n  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: attrs>=17.3.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp) (22.1.0)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna>=2.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\nInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp\nSuccessfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.4 yarl-1.9.4\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting aiofiles\n  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\nInstalling collected packages: aiofiles\nSuccessfully installed aiofiles-23.2.1\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting openpyxl\n  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting et-xmlfile\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 3,
      "block_group": "c84d950980c84077a8da3dcab0c2cb77"
    },
    {
      "cell_type": "code",
      "source": "sheet_data",
      "metadata": {
        "source_hash": null,
        "execution_start": 1703696279709,
        "execution_millis": 5541,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "29427c77b7564e708a938fe54fcb32e0",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sheet_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msheet_data\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sheet_data' is not defined"
          ]
        }
      ],
      "execution_count": null,
      "block_group": "198e0575f5114267bb03dbfad9051b62"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "acb9786434f74ebdbf5263d9c0b37e8a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "0878f124599d4f1a840a5e61bcb92015"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=8c9bc8ce-5c4b-4022-b35b-f80ff82465a6' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "b53a26c9337c412a8bd9a52356d2e2dd",
    "deepnote_persisted_session": {
      "createdAt": "2023-12-27T17:15:14.638Z"
    },
    "deepnote_execution_queue": []
  }
}